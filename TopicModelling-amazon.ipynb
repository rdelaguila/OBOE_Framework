{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datatable\n",
    "import itertools as it\n",
    "from owlready2 import *\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "import pickle\n",
    "from bayes_opt import BayesianOptimization\n",
    "#para depurar los que letras que no están en el codigo ascii\n",
    "import unicodedata\n",
    "import functools\n",
    "import spacy\n",
    "import stanfordnlp\n",
    "#from spacy_stanfordnlp import StanfordNLPLanguage\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import metrics\n",
    "#from wiki_dump_reader import Cleaner, iterate\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize,scale\n",
    "\n",
    "\n",
    "\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import random\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#Operaciones con gráficos\n",
    "import scattertext as st\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool, LabelSet, ColumnDataSource, Range1d\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "#from gensim.models import CoherenceModel\n",
    "#from gensim.models import HdpModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from multiprocessing import  Pool\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../config/config.json', 'r') as f:\n",
    "    config = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('../data/interim/amazon/corpus_normalizado_t_original.csv')\n",
    "indices = pd.read_csv('../data/interim/amazon/corpus1_50k.csv')\n",
    "indices = indices['Unnamed: 0']\n",
    "corpus = corpus.loc[indices,:]\n",
    "corpus = corpus.iloc[:,[3,5]]\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalizado</th>\n",
       "      <th>target_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>book recommend friend finally read year late w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>need question know child love impair identify ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>keep interest way definitely read jacob s moth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>c w gortner favorite author time wow queen rel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>love book like jk alex delaware series problem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          normalizado  target_original\n",
       "9   book recommend friend finally read year late w...                1\n",
       "10  need question know child love impair identify ...                1\n",
       "11  keep interest way definitely read jacob s moth...                1\n",
       "51  c w gortner favorite author time wow queen rel...                1\n",
       "61  love book like jk alex delaware series problem...                1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv('../data/interim/amazon/corpus_normalizado.csv')\n",
    "\n",
    "interimpath = '../data/interim/amazon/corpus_normalizado.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/interim/amazon/corpus_normalizado.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interimpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>normalizado</th>\n",
       "      <th>target_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>book recommend friend finally read year late w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>need question know child love impair identify ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>keep interest way definitely read jacob s moth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>c w gortner favorite author time wow queen rel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>love book like jk alex delaware series problem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        normalizado  \\\n",
       "0           9  book recommend friend finally read year late w...   \n",
       "1          10  need question know child love impair identify ...   \n",
       "2          11  keep interest way definitely read jacob s moth...   \n",
       "3          51  c w gortner favorite author time wow queen rel...   \n",
       "4          61  love book like jk alex delaware series problem...   \n",
       "\n",
       "   target_original  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(interimpath)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book recommend friend finally read year late w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need question know child love impair identify ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keep interest way definitely read jacob s moth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c w gortner favorite author time wow queen rel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love book like jk alex delaware series problem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  Target\n",
       "0  book recommend friend finally read year late w...       1\n",
       "1  need question know child love impair identify ...       1\n",
       "2  keep interest way definitely read jacob s moth...       1\n",
       "3  c w gortner favorite author time wow queen rel...       1\n",
       "4  love book like jk alex delaware series problem...       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corpus = corpus.loc[:,['normalizado','target_original']]\n",
    "corpus.columns=['cleaned_text','Target']\n",
    "corpus.Target = corpus.Target.astype(int)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.append('be')\n",
    "stop_words.append('don')\n",
    "from sklearn.feature_extraction import text\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"be\",\"don\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer LDA, no vamos a utilizar todo el corpus. Seleccionaremos aleatoriamente 100.000 textos de clase positiva y sin clasificar para posteriormente hacer LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/tfidf_object']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=50,max_df=0.95, stop_words=['english','spanish'], ngram_range=(1,1))\n",
    "#por ahora no pongo ngramas\n",
    "joblib.dump(tfidf,'../objects/'+'amazon'+'/tfidf_object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(corpus.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x3468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1209577 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anteriormente, hemos realizado un grid search. Vamos a utilizar los parámetros que mejor funcionaron en su momento\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "search_params = { 'topic_word_prior':[0.5,0.7,0,0.95,0.99], # hemos partido de 0.5, 0.7, 0,9, 0.95 y 0.99\n",
    "                 'doc_topic_prior':[0.01,0.015,0.05,0.3,0.5]}\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=2,random_state=42,n_jobs=10,batch_size=50000,  evaluate_every = 100,  mean_change_tol=0.05)\n",
    "\n",
    "model = GridSearchCV(LDA, param_grid=search_params)\n",
    "from sklearn.utils import parallel_backend\n",
    "i = time.time()\n",
    "with parallel_backend('threading'):\n",
    "    # Do the Grid Search\n",
    "    model.fit(dtm)\n",
    "\n",
    "    joblib.dump(model, 'lda_model4.pkl')\n",
    "\n",
    "f = time.time()\n",
    "elapsed = f - i\n",
    "print('transcurridos {0} s'.format(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ya sabemos que nuestro mejor modelo tiene unos priors determinados, \n",
    "joblib.dump(modelo,'../objects/'+config.get('corpus')+'/modelo_gridsearch.final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/lda.final']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=2,random_state=42,topic_word_prior = 0.95,doc_topic_prior = 0.015,n_jobs=10,max_iter=250,mean_change_tol=0.05)\n",
    "joblib.dump(LDA,'../objects/amazon/lda.final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=50,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['english', 'spanish'], strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf = joblib.load('tfidf')\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = corpus.iloc[:,0]\n",
    "texts = list(texts.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1080p',\n",
       " '11',\n",
       " '12',\n",
       " '120',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '150',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '20th',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '250',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '2nd',\n",
       " '30',\n",
       " '300',\n",
       " '32',\n",
       " '35',\n",
       " '39',\n",
       " '3rd',\n",
       " '40',\n",
       " '400',\n",
       " '45',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '55',\n",
       " '5th',\n",
       " '60',\n",
       " '64',\n",
       " '65',\n",
       " '70',\n",
       " '75',\n",
       " '80',\n",
       " '800',\n",
       " '90',\n",
       " '95',\n",
       " '99',\n",
       " 'aa',\n",
       " 'abandon',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorb',\n",
       " 'abuse',\n",
       " 'abusive',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accompany',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accuse',\n",
       " 'achieve',\n",
       " 'acknowledge',\n",
       " 'acquire',\n",
       " 'act',\n",
       " 'action',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adapt',\n",
       " 'adapter',\n",
       " 'add',\n",
       " 'addict',\n",
       " 'addiction',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'adequate',\n",
       " 'adhesive',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjustment',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'adopt',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'advertise',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afraid',\n",
       " 'african',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agendum',\n",
       " 'agent',\n",
       " 'aggressive',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'al',\n",
       " 'alarm',\n",
       " 'albeit',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alien',\n",
       " 'align',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'allow',\n",
       " 'ally',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'alpha',\n",
       " 'alter',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'aluminum',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amuse',\n",
       " 'analysis',\n",
       " 'ancient',\n",
       " 'andrew',\n",
       " 'android',\n",
       " 'anecdote',\n",
       " 'angel',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'animal',\n",
       " 'ann',\n",
       " 'anne',\n",
       " 'annoy',\n",
       " 'answer',\n",
       " 'antenna',\n",
       " 'anthology',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'aperture',\n",
       " 'apocalypse',\n",
       " 'apocalyptic',\n",
       " 'app',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'april',\n",
       " 'arc',\n",
       " 'arch',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'arise',\n",
       " 'arm',\n",
       " 'army',\n",
       " 'arp',\n",
       " 'arrange',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'art',\n",
       " 'article',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artwork',\n",
       " 'ash',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'ass',\n",
       " 'assassin',\n",
       " 'assemble',\n",
       " 'assembly',\n",
       " 'assign',\n",
       " 'assist',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'assume',\n",
       " 'assure',\n",
       " 'asus',\n",
       " 'atmosphere',\n",
       " 'attach',\n",
       " 'attachment',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attract',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'audiophile',\n",
       " 'aunt',\n",
       " 'authentic',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'auto',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avid',\n",
       " 'avoid',\n",
       " 'await',\n",
       " 'awaken',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'backdrop',\n",
       " 'background',\n",
       " 'backlight',\n",
       " 'backpack',\n",
       " 'backstory',\n",
       " 'backup',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'bake',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'band',\n",
       " 'bang',\n",
       " 'bank',\n",
       " 'banter',\n",
       " 'bar',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barrel',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'bass',\n",
       " 'bat',\n",
       " 'bathroom',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'bay',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'bead',\n",
       " 'bear',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'beg',\n",
       " 'begin',\n",
       " 'beginner',\n",
       " 'behave',\n",
       " 'behavior',\n",
       " 'being',\n",
       " 'belief',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believer',\n",
       " 'bell',\n",
       " 'belong',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'bench',\n",
       " 'bend',\n",
       " 'benefit',\n",
       " 'bet',\n",
       " 'betrayal',\n",
       " 'beware',\n",
       " 'bias',\n",
       " 'bible',\n",
       " 'biblical',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'bind',\n",
       " 'bio',\n",
       " 'biography',\n",
       " 'bird',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'blade',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'bleed',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'blog',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'bluetooth',\n",
       " 'blur',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'boil',\n",
       " 'bold',\n",
       " 'bolt',\n",
       " 'bomb',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'book',\n",
       " 'bookshelf',\n",
       " 'bookstore',\n",
       " 'boom',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'border',\n",
       " 'bore',\n",
       " 'borg',\n",
       " 'borrow',\n",
       " 'bose',\n",
       " 'boss',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bounce',\n",
       " 'bow',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'bracket',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brave',\n",
       " 'bravo',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breed',\n",
       " 'breeze',\n",
       " 'brian',\n",
       " 'bride',\n",
       " 'bridge',\n",
       " 'brief',\n",
       " 'bright',\n",
       " 'brightness',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'british',\n",
       " 'broad',\n",
       " 'brother',\n",
       " 'brown',\n",
       " 'browse',\n",
       " 'brush',\n",
       " 'brutal',\n",
       " 'bubble',\n",
       " 'buck',\n",
       " 'bud',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buff',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'bulk',\n",
       " 'bulky',\n",
       " 'bully',\n",
       " 'bump',\n",
       " 'bunch',\n",
       " 'bundle',\n",
       " 'burn',\n",
       " 'bury',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'cabinet',\n",
       " 'cable',\n",
       " 'cage',\n",
       " 'cake',\n",
       " 'california',\n",
       " 'call',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'camcorder',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'can',\n",
       " 'canada',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'candy',\n",
       " 'canon',\n",
       " 'canvas',\n",
       " 'cap',\n",
       " 'capability',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'captain',\n",
       " 'captivate',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cardboard',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'carpet',\n",
       " 'carry',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cast',\n",
       " 'casual',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'category',\n",
       " 'cathedral',\n",
       " 'catholic',\n",
       " 'cause',\n",
       " 'caution',\n",
       " 'cave',\n",
       " 'cd',\n",
       " 'celebrate',\n",
       " 'cell',\n",
       " 'cent',\n",
       " 'center',\n",
       " 'central',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chain',\n",
       " 'chair',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'channel',\n",
       " 'chaos',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'characteristic',\n",
       " 'characterization',\n",
       " 'charge',\n",
       " 'charger',\n",
       " 'charles',\n",
       " 'charm',\n",
       " 'chart',\n",
       " 'chase',\n",
       " 'cheap',\n",
       " 'cheaply',\n",
       " 'cheat',\n",
       " 'check',\n",
       " 'cheer',\n",
       " 'cheesy',\n",
       " 'chemical',\n",
       " 'chemistry',\n",
       " 'chest',\n",
       " 'chew',\n",
       " 'chicago',\n",
       " 'chicken',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'chill',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chip',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'christmas',\n",
       " 'chronicle',\n",
       " 'church',\n",
       " 'circle',\n",
       " 'circumstance',\n",
       " 'citizen',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'civilization',\n",
       " 'claim',\n",
       " 'clarity',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'classroom',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'click',\n",
       " 'client',\n",
       " 'cliff',\n",
       " 'cliffhanger',\n",
       " 'climax',\n",
       " 'climb',\n",
       " 'clip',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closely',\n",
       " 'closure',\n",
       " 'cloth',\n",
       " 'clothe',\n",
       " 'cloud',\n",
       " 'club',\n",
       " 'clue',\n",
       " 'cm',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'coal',\n",
       " 'coast',\n",
       " 'coaster',\n",
       " 'coat',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'collapse',\n",
       " 'collar',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'collector',\n",
       " 'college',\n",
       " 'color',\n",
       " 'colorful',\n",
       " 'com',\n",
       " 'combat',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'combo',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comfortably',\n",
       " 'comfy',\n",
       " 'comic',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'commentary',\n",
       " 'commercial',\n",
       " 'commit',\n",
       " 'commitment',\n",
       " 'common',\n",
       " 'communicate',\n",
       " 'communication',\n",
       " 'community',\n",
       " 'compact',\n",
       " 'companion',\n",
       " 'company',\n",
       " 'comparable',\n",
       " 'compare',\n",
       " 'comparison',\n",
       " 'compartment',\n",
       " 'compassion',\n",
       " 'compatible',\n",
       " 'compel',\n",
       " 'compete',\n",
       " 'competition',\n",
       " 'complain',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complexity',\n",
       " 'complicate',\n",
       " 'compliment',\n",
       " 'complimentary',\n",
       " 'component',\n",
       " 'comprehensive',\n",
       " 'compromise',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'concentrate',\n",
       " 'concept',\n",
       " 'concern',\n",
       " 'concise',\n",
       " 'conclude',\n",
       " 'conclusion',\n",
       " 'condition',\n",
       " 'confess',\n",
       " 'confession',\n",
       " 'confidence',\n",
       " 'confident',\n",
       " 'configuration',\n",
       " 'configure',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'confront',\n",
       " 'confuse',\n",
       " 'confusion',\n",
       " 'connect',\n",
       " 'connection',\n",
       " 'connectivity',\n",
       " 'connector',\n",
       " 'consciousness',\n",
       " 'consequence',\n",
       " 'conservative',\n",
       " 'consider',\n",
       " 'consideration',\n",
       " 'consist',\n",
       " 'consistent',\n",
       " 'consistently',\n",
       " 'conspiracy',\n",
       " 'constant',\n",
       " 'constantly',\n",
       " 'construct',\n",
       " 'construction',\n",
       " 'consume',\n",
       " 'consumer',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'container',\n",
       " 'contemporary',\n",
       " 'content',\n",
       " 'context',\n",
       " 'continuation',\n",
       " 'continue',\n",
       " 'contrast',\n",
       " 'contribute',\n",
       " 'contribution',\n",
       " 'contrive',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'convenience',\n",
       " 'convenient',\n",
       " 'conversation',\n",
       " 'convert',\n",
       " 'convey',\n",
       " 'convince',\n",
       " 'cook',\n",
       " 'cookbook',\n",
       " 'cooker',\n",
       " 'cool',\n",
       " 'cop',\n",
       " 'cope',\n",
       " 'copy',\n",
       " 'cord',\n",
       " 'core',\n",
       " 'corner',\n",
       " 'correct',\n",
       " 'correctly',\n",
       " 'corrupt',\n",
       " 'cost',\n",
       " 'cotton',\n",
       " 'couch',\n",
       " 'couldn',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'courage',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cousin',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'cowboy',\n",
       " 'cpu',\n",
       " 'cr',\n",
       " 'crack',\n",
       " 'craft',\n",
       " 'crank',\n",
       " 'crap',\n",
       " 'crash',\n",
       " 'crazy',\n",
       " 'create',\n",
       " 'creation',\n",
       " 'creative',\n",
       " 'creativity',\n",
       " 'creature',\n",
       " 'credit',\n",
       " 'creepy',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'crisis',\n",
       " 'crisp',\n",
       " 'critical',\n",
       " 'criticism',\n",
       " 'crochet',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'crow',\n",
       " 'crowd',\n",
       " 'crown',\n",
       " 'cruel',\n",
       " 'crush',\n",
       " 'cry',\n",
       " 'crystal',\n",
       " 'cultural',\n",
       " 'culture',\n",
       " 'cup',\n",
       " 'cure',\n",
       " 'curiosity',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'curse',\n",
       " 'curve',\n",
       " 'cushion',\n",
       " 'custom',\n",
       " 'customer',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cycle',\n",
       " 'dad',\n",
       " 'daily',\n",
       " 'damage',\n",
       " 'damn',\n",
       " 'dan',\n",
       " 'dance',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'daniel',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'darkness',\n",
       " 'darn',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'datum',\n",
       " 'daughter',\n",
       " 'david',\n",
       " 'day',\n",
       " 'dc',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deadly',\n",
       " 'deal',\n",
       " 'dealer',\n",
       " 'dean',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'debate',\n",
       " 'debut',\n",
       " 'decade',\n",
       " 'decent',\n",
       " 'decide',\n",
       " 'decision',\n",
       " 'dedicate',\n",
       " 'deep',\n",
       " 'deeply',\n",
       " 'default',\n",
       " 'defeat',\n",
       " 'defective',\n",
       " 'defend',\n",
       " 'defense',\n",
       " 'define',\n",
       " 'definite',\n",
       " 'definitely',\n",
       " 'definition',\n",
       " 'degree',\n",
       " 'delay',\n",
       " 'delete',\n",
       " 'delicious',\n",
       " 'delight',\n",
       " 'delightful',\n",
       " 'deliver',\n",
       " 'delivery',\n",
       " 'dell',\n",
       " 'delve',\n",
       " 'demand',\n",
       " 'demon',\n",
       " 'demonstrate',\n",
       " 'deny',\n",
       " 'department',\n",
       " 'depend',\n",
       " 'depict',\n",
       " 'depiction',\n",
       " 'depress',\n",
       " 'depth',\n",
       " 'describe',\n",
       " 'description',\n",
       " 'descriptive',\n",
       " 'desert',\n",
       " 'deserve',\n",
       " 'design',\n",
       " 'designer',\n",
       " 'desire',\n",
       " 'desk',\n",
       " 'desktop',\n",
       " 'desperate',\n",
       " 'desperately',\n",
       " 'despite',\n",
       " 'destiny',\n",
       " 'destroy',\n",
       " 'destruction',\n",
       " 'detail',\n",
       " 'detect',\n",
       " 'detective',\n",
       " 'determination',\n",
       " 'determine',\n",
       " 'develop',\n",
       " 'develope',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devil',\n",
       " 'devote',\n",
       " 'devour',\n",
       " 'diagram',\n",
       " 'dial',\n",
       " 'dialog',\n",
       " 'dialogue',\n",
       " 'diary',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'diet',\n",
       " 'differ',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'difficult',\n",
       " 'difficulty',\n",
       " 'dig',\n",
       " 'digital',\n",
       " 'dim',\n",
       " 'dimension',\n",
       " 'dimensional',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'directly',\n",
       " 'director',\n",
       " 'dirt',\n",
       " 'dirty',\n",
       " 'disagree',\n",
       " 'disappear',\n",
       " 'disappoint',\n",
       " 'disappointment',\n",
       " 'disaster',\n",
       " 'disc',\n",
       " 'discipline',\n",
       " 'disclaimer',\n",
       " 'disclosure',\n",
       " 'disconnect',\n",
       " 'discount',\n",
       " 'discover',\n",
       " 'discovery',\n",
       " 'discuss',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disgust',\n",
       " 'dish',\n",
       " 'disk',\n",
       " 'dislike',\n",
       " 'disorder',\n",
       " 'display',\n",
       " 'distance',\n",
       " 'distant',\n",
       " 'distinct',\n",
       " 'distort',\n",
       " 'distract',\n",
       " 'disturb',\n",
       " 'diva',\n",
       " 'diverse',\n",
       " 'divide',\n",
       " 'divorce',\n",
       " 'dock',\n",
       " 'doctor',\n",
       " 'document',\n",
       " 'documentation',\n",
       " 'doesn',\n",
       " 'dog',\n",
       " 'dollar',\n",
       " 'dominate',\n",
       " 'don',\n",
       " 'doom',\n",
       " 'door',\n",
       " 'dose',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downright',\n",
       " 'downside',\n",
       " 'dozen',\n",
       " 'dp',\n",
       " 'dr',\n",
       " 'drag',\n",
       " 'dragon',\n",
       " 'drain',\n",
       " 'drama',\n",
       " 'dramatic',\n",
       " 'draw',\n",
       " 'drawback',\n",
       " 'drawing',\n",
       " 'dream',\n",
       " 'dress',\n",
       " 'drill',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drop',\n",
       " 'drug',\n",
       " 'dry',\n",
       " 'dslr',\n",
       " 'dual',\n",
       " 'dull',\n",
       " 'dumb',\n",
       " 'dump',\n",
       " 'durability',\n",
       " 'durable',\n",
       " 'dust',\n",
       " 'duty',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " terminado en 40.55226482550303 min\n"
     ]
    }
   ],
   "source": [
    "i = time.time()\n",
    "from sklearn.utils import parallel_backend\n",
    "i = time.time()\n",
    "with parallel_backend('threading'):\n",
    "    modelo = LDA.fit(dtm)\n",
    "print (' terminado en {0} min'.format((time.time() - i) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/tfidf.final']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(modelo,'../objects/amazon/final_lda_fit.final.pck')\n",
    "joblib.dump(feature_names,'../objects/amazon/feature_names.final')\n",
    "joblib.dump(tfidf,'../objects/amazon/tfidf.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo = joblib.load('final_lda.pck')\n",
    "#modelo = joblib.load('final_lda.final.pck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.externals import joblib\n",
    "modelo = joblib.load('../objects/amazon/final_lda_fit.final.pck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = joblib.load('../objects/amazon/tfidf_object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = joblib.load('../objects/amazon/feature_names.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['fit', 'dog', 'need', 'nice', 'easy', 'like', 'love', 'quality', 'price', 'buy', 'use', 'good', 'product', 'great', 'work']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['time', 'end', 'interest', 'author', 'enjoy', 'write', 'like', 'character', 'good', 'great', 'series', 'love', 'story', 'read', 'book']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(modelo.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([feature_names[i] for i in topic.argsort()[-15:]])\n",
    "    joblib.dump([feature_names[i] for i in topic.argsort()],'../objects/amazon/topics'+str(index))\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['fit', 'dog', 'need', 'nice', 'easy', 'like', 'love', 'quality', 'price', 'buy', 'use', 'good', 'product', 'great', 'work']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['time', 'end', 'interest', 'author', 'enjoy', 'write', 'like', 'character', 'good', 'great', 'series', 'love', 'story', 'read', 'book']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(modelo.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    joblib.dump([tfidf.get_feature_names()[i] for i in topic.argsort()],'../objects/amazon/topics'+str(index))\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dtm as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds = pd.DataFrame(dtm.todense(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/dtm.final']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(dtm,'../objects/amazon/dtm.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/dtm_pds']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pds,'../objects/amazon/dtm_pds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldares = modelo.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/ldares_transform.final']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(ldares,'../objects/amazon/ldares_transform.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldares = joblib.load('../objects/amazon/ldares_transform.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00303975, 0.99696025],\n",
       "       [0.00384368, 0.99615632],\n",
       "       [0.00463416, 0.99536584],\n",
       "       ...,\n",
       "       [0.99515363, 0.00484637],\n",
       "       [0.99648833, 0.00351167],\n",
       "       [0.99373686, 0.00626314]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv(max_depth, ntrees,min_rows,eta,learn_rate,sample_rate,colsample_bytree,reg_lambda,reg_alpha,train):\n",
    "    \"\"\"SVC cross validation.\n",
    "    This function will instantiate a SVC classifier with parameters C and\n",
    "    gamma. Combined with data and targets this will in turn be used to perform\n",
    "    cross validation. The result of cross validation is returned.\n",
    "    Our goal is to find combinations of C and gamma that maximizes the roc_auc\n",
    "    metric.\n",
    "    \"\"\"\n",
    "    params = {'max_depth': int(max_depth), 'ntrees': int(ntrees),'min_rows':int(min_rows),'eta':float(eta),\n",
    "              'learn_rate':float(learn_rate),'sample_rate':float(sample_rate),\n",
    "              'colsample_bytree':float(colsample_bytree),'reg_lambda':float(reg_lambda),'reg_alpha':float(reg_alpha)}\n",
    "      \n",
    "    estimator = H2OXGBoostEstimator(**params)\n",
    "    train['target'] = train['target'].asfactor()\n",
    "    test['target'] = test['target'].asfactor()\n",
    "    \n",
    "    estimator.train(x=['A','B'],y='target',training_frame=train,validation_frame=test)\n",
    "    return estimator.auc()\n",
    "\n",
    "def optimize_xgb(train):\n",
    "    \"\"\"Apply Bayesian Optimization to XGB parameters.\"\"\"\n",
    "    def xgb_crossval(max_depth, ntrees,min_rows,eta,learn_rate,sample_rate,colsample_bytree,reg_lambda,reg_alpha,train=train):\n",
    "        \"\"\"Wrapper of SVC cross validation.\n",
    "        Notice how we transform between regular and log scale. While this\n",
    "        is not technically necessary, it greatly improves the performance\n",
    "        of the optimizer.\n",
    "        \"\"\"\n",
    "        return xgb_cv(max_depth, ntrees,min_rows,eta,learn_rate,sample_rate,colsample_bytree,reg_lambda,reg_alpha,train)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=xgb_crossval,\n",
    "        \n",
    "        pbounds={\"max_depth\": (4, 40), \"ntrees\": (50,400),'min_rows':(2,50),'eta':(0.01,1),\n",
    "                 'learn_rate':(0.01,1),'sample_rate':(0.1,0.5),'colsample_bytree':(0.2,0.8),\n",
    "                 'reg_lambda':(0,1),'reg_alpha':(0,1)},\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    )\n",
    "    modelos = optimizer.maximize(n_iter=10)\n",
    "    return modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(ldares,columns=['A','B'])\n",
    "data['Target'] = corpus.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/interim/amazon/datatopic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator, H2ODeepLearningEstimator,H2ORandomForestEstimator,H2OSupportVectorMachineEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.6\" 2020-01-14; OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1); OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing)\n",
      "  Starting server from /home/rdelaguila/anaconda3/lib/python3.7/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpvix50en2\n",
      "  JVM stdout: /tmp/tmpvix50en2/h2o_rdelaguila_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpvix50en2/h2o_rdelaguila_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (5 months and 10 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>5 months and 10 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_rdelaguila_2bi8mt</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>14 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>12</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>12</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.4 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Europe/Berlin\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.0.3\n",
       "H2O cluster version age:    5 months and 10 days !!!\n",
       "H2O cluster name:           H2O_from_python_rdelaguila_2bi8mt\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    14 Gb\n",
       "H2O cluster total cores:    12\n",
       "H2O cluster allowed cores:  12\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.7.4 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "h2o.init(max_mem_size='14G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "frame = h2o.upload_file('../data/interim/amazon/datatopic.csv',sep=',',header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  C1</th><th style=\"text-align: right;\">         A</th><th style=\"text-align: right;\">       B</th><th style=\"text-align: right;\">  Target</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.00303975</td><td style=\"text-align: right;\">0.99696 </td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">0.00384368</td><td style=\"text-align: right;\">0.996156</td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">0.00463416</td><td style=\"text-align: right;\">0.995366</td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   3</td><td style=\"text-align: right;\">0.00157954</td><td style=\"text-align: right;\">0.99842 </td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   4</td><td style=\"text-align: right;\">0.0041268 </td><td style=\"text-align: right;\">0.995873</td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   5</td><td style=\"text-align: right;\">0.00440491</td><td style=\"text-align: right;\">0.995595</td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   6</td><td style=\"text-align: right;\">0.0048532 </td><td style=\"text-align: right;\">0.995147</td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   7</td><td style=\"text-align: right;\">0.00433073</td><td style=\"text-align: right;\">0.995669</td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   8</td><td style=\"text-align: right;\">0.00413952</td><td style=\"text-align: right;\">0.99586 </td><td style=\"text-align: right;\">       1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   9</td><td style=\"text-align: right;\">0.00371882</td><td style=\"text-align: right;\">0.996281</td><td style=\"text-align: right;\">       1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frame.drop('C1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['target']=frame['Target'].asfactor()\n",
    "train,test=frame.split_frame(ratios=[.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    | learn_... | max_depth | min_rows  |  ntrees   | reg_alpha | reg_la... | sample... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  1        |  0.9421   |  0.3149   |  0.6259   |  0.4434   |  32.27    |  39.44    |  145.4    |  0.2765   |  0.8019   |  0.4833   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  2        |  0.9226   |  0.7256   |  0.3642   |  0.506    |  28.6     |  36.21    |  179.6    |  0.5612   |  0.5031   |  0.1055   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  3        |  0.9663   |  0.6637   |  0.8838   |  0.3712   |  26.15    |  5.618    |  179.1    |  0.9331   |  0.6514   |  0.2589   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  4        |  0.9517   |  0.6732   |  0.3237   |  0.5724   |  35.29    |  22.94    |  330.8    |  0.1438   |  0.7043   |  0.3818   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  5        |  0.9761   |  0.3313   |  0.9256   |  0.4477   |  36.74    |  4.871    |  114.5    |  0.04736  |  0.6749   |  0.3378   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  6        |  0.9214   |  0.8      |  0.01     |  1.0      |  4.0      |  2.0      |  400.0    |  1.0      |  8.571e-0 |  0.1      |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  7        |  0.9254   |  0.5017   |  0.4651   |  0.1934   |  6.692    |  47.41    |  53.36    |  0.4854   |  0.9751   |  0.4363   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  8        |  0.9508   |  0.2451   |  0.09652  |  0.6351   |  38.83    |  3.124    |  112.4    |  0.8768   |  0.3945   |  0.3741   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  9        |  0.9581   |  0.5917   |  0.9998   |  0.1155   |  38.01    |  48.92    |  397.6    |  0.573    |  0.1422   |  0.493    |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  10       |  0.9222   |  0.2      |  1.0      |  1.0      |  4.0      |  50.0     |  309.0    |  0.0      |  1.0      |  0.1      |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  11       |  0.9292   |  0.2972   |  0.4451   |  0.8968   |  4.277    |  3.219    |  108.1    |  0.3985   |  0.3019   |  0.2834   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  12       |  0.9204   |  0.8      |  1.0      |  0.01     |  4.0      |  2.0      |  264.7    |  0.0      |  1.0      |  0.1      |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  13       |  0.984    |  0.633    |  0.6331   |  0.4182   |  39.22    |  4.773    |  393.5    |  0.06257  |  0.8628   |  0.2508   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  14       |  0.9301   |  0.714    |  0.1521   |  0.5572   |  38.81    |  45.81    |  266.8    |  0.03721  |  0.8876   |  0.3351   |\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "|  15       |  0.9237   |  0.5161   |  0.9833   |  0.1849   |  39.86    |  43.79    |  53.28    |  0.4918   |  0.8393   |  0.2142   |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "modelos = optimize_xgb(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "params = {'max_depth': int(39), 'ntrees': int(393),'min_rows':int(5),'eta':float(0.6331),\n",
    "              'learn_rate':float(0.418),'sample_rate':float(0.2508),\n",
    "              'colsample_bytree':float(0.633),'reg_lambda':float(0.8628),'reg_alpha':float(0.06257)}\n",
    "      \n",
    "estimator = H2OXGBoostEstimator(**params)\n",
    "train['target'] = train['target'].asfactor()\n",
    "test['target'] = test['target'].asfactor()\n",
    "    \n",
    "estimator.train(x=['A','B'],y='target',training_frame=train,validation_frame=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = estimator.model_performance(test)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.11759757685304478\n",
      "RMSE: 0.3429250309514381\n",
      "LogLoss: 0.44860056941989046\n",
      "Mean Per-Class Error: 0.13632630714481242\n",
      "AUC: 0.8978100929556074\n",
      "AUCPR: 0.8510384917558662\n",
      "Gini: 0.7956201859112149\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2780561298131943: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4929.0</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>(1253.0/6182.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>453.0</td>\n",
       "      <td>5904.0</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>(453.0/6357.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>5382.0</td>\n",
       "      <td>7157.0</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>(1706.0/12539.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0       1   Error               Rate\n",
       "0      0  4929.0  1253.0  0.2027    (1253.0/6182.0)\n",
       "1      1   453.0  5904.0  0.0713     (453.0/6357.0)\n",
       "2  Total  5382.0  7157.0  0.1361   (1706.0/12539.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>value</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max f1</td>\n",
       "      <td>0.278056</td>\n",
       "      <td>0.873761</td>\n",
       "      <td>283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max f2</td>\n",
       "      <td>0.083471</td>\n",
       "      <td>0.916063</td>\n",
       "      <td>341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max f0point5</td>\n",
       "      <td>0.538910</td>\n",
       "      <td>0.852487</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max accuracy</td>\n",
       "      <td>0.311470</td>\n",
       "      <td>0.864503</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max precision</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max recall</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max specificity</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>0.999353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max absolute_mcc</td>\n",
       "      <td>0.300916</td>\n",
       "      <td>0.733600</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max min_per_class_accuracy</td>\n",
       "      <td>0.568096</td>\n",
       "      <td>0.848107</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max mean_per_class_accuracy</td>\n",
       "      <td>0.311470</td>\n",
       "      <td>0.863674</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max tns</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>6178.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max fns</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>6318.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max fps</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>6182.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max tps</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>6357.000000</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max tnr</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>0.999353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max fnr</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max fpr</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max tpr</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  threshold        value    idx\n",
       "0                        max f1   0.278056     0.873761  283.0\n",
       "1                        max f2   0.083471     0.916063  341.0\n",
       "2                  max f0point5   0.538910     0.852487  211.0\n",
       "3                  max accuracy   0.311470     0.864503  275.0\n",
       "4                 max precision   0.999501     0.906977    0.0\n",
       "5                    max recall   0.000922     1.000000  398.0\n",
       "6               max specificity   0.999501     0.999353    0.0\n",
       "7              max absolute_mcc   0.300916     0.733600  278.0\n",
       "8    max min_per_class_accuracy   0.568096     0.848107  202.0\n",
       "9   max mean_per_class_accuracy   0.311470     0.863674  275.0\n",
       "10                      max tns   0.999501  6178.000000    0.0\n",
       "11                      max fns   0.999501  6318.000000    0.0\n",
       "12                      max fps   0.000534  6182.000000  399.0\n",
       "13                      max tps   0.000922  6357.000000  398.0\n",
       "14                      max tnr   0.999501     0.999353    0.0\n",
       "15                      max fnr   0.999501     0.993865    0.0\n",
       "16                      max fpr   0.000534     1.000000  399.0\n",
       "17                      max tpr   0.000922     1.000000  398.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 50.70 %, avg score: 50.64 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cumulative_data_fraction</th>\n",
       "      <th>lower_threshold</th>\n",
       "      <th>lift</th>\n",
       "      <th>cumulative_lift</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>score</th>\n",
       "      <th>cumulative_response_rate</th>\n",
       "      <th>cumulative_score</th>\n",
       "      <th>capture_rate</th>\n",
       "      <th>cumulative_capture_rate</th>\n",
       "      <th>gain</th>\n",
       "      <th>cumulative_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.998292</td>\n",
       "      <td>1.788986</td>\n",
       "      <td>1.788986</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.998909</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.998909</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>78.898559</td>\n",
       "      <td>78.898559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>0.997351</td>\n",
       "      <td>1.612417</td>\n",
       "      <td>1.701740</td>\n",
       "      <td>0.817460</td>\n",
       "      <td>0.997867</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.998394</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.034608</td>\n",
       "      <td>61.241701</td>\n",
       "      <td>70.173994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0.030066</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>1.778458</td>\n",
       "      <td>1.726566</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.996884</td>\n",
       "      <td>0.875332</td>\n",
       "      <td>0.997906</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.051911</td>\n",
       "      <td>77.845772</td>\n",
       "      <td>72.656638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>0.040195</td>\n",
       "      <td>0.995519</td>\n",
       "      <td>1.677377</td>\n",
       "      <td>1.714171</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>0.995992</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.997423</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>67.737716</td>\n",
       "      <td>71.417148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.994547</td>\n",
       "      <td>1.828144</td>\n",
       "      <td>1.736530</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.995005</td>\n",
       "      <td>0.880383</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>0.017933</td>\n",
       "      <td>0.086833</td>\n",
       "      <td>82.814412</td>\n",
       "      <td>73.652975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.989499</td>\n",
       "      <td>1.730238</td>\n",
       "      <td>1.733384</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.992002</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.994476</td>\n",
       "      <td>0.086519</td>\n",
       "      <td>0.173352</td>\n",
       "      <td>73.023797</td>\n",
       "      <td>73.338386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0.150251</td>\n",
       "      <td>0.983800</td>\n",
       "      <td>1.700082</td>\n",
       "      <td>1.722248</td>\n",
       "      <td>0.861905</td>\n",
       "      <td>0.986855</td>\n",
       "      <td>0.873142</td>\n",
       "      <td>0.991927</td>\n",
       "      <td>0.085418</td>\n",
       "      <td>0.258770</td>\n",
       "      <td>70.008240</td>\n",
       "      <td>72.224802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>0.200973</td>\n",
       "      <td>0.975951</td>\n",
       "      <td>1.730564</td>\n",
       "      <td>1.724347</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>0.979845</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.988878</td>\n",
       "      <td>0.087777</td>\n",
       "      <td>0.346547</td>\n",
       "      <td>73.056443</td>\n",
       "      <td>72.434693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0.300024</td>\n",
       "      <td>0.942369</td>\n",
       "      <td>1.675489</td>\n",
       "      <td>1.708217</td>\n",
       "      <td>0.849436</td>\n",
       "      <td>0.962482</td>\n",
       "      <td>0.866029</td>\n",
       "      <td>0.980163</td>\n",
       "      <td>0.165959</td>\n",
       "      <td>0.512506</td>\n",
       "      <td>67.548890</td>\n",
       "      <td>70.821676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0.400112</td>\n",
       "      <td>0.857930</td>\n",
       "      <td>1.695854</td>\n",
       "      <td>1.705124</td>\n",
       "      <td>0.859761</td>\n",
       "      <td>0.907741</td>\n",
       "      <td>0.864461</td>\n",
       "      <td>0.962047</td>\n",
       "      <td>0.169734</td>\n",
       "      <td>0.682240</td>\n",
       "      <td>69.585380</td>\n",
       "      <td>70.512418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>0.500040</td>\n",
       "      <td>0.588180</td>\n",
       "      <td>1.597812</td>\n",
       "      <td>1.683679</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.741201</td>\n",
       "      <td>0.853589</td>\n",
       "      <td>0.917913</td>\n",
       "      <td>0.159667</td>\n",
       "      <td>0.841907</td>\n",
       "      <td>59.781194</td>\n",
       "      <td>68.367884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.599968</td>\n",
       "      <td>0.159786</td>\n",
       "      <td>1.045268</td>\n",
       "      <td>1.577348</td>\n",
       "      <td>0.529928</td>\n",
       "      <td>0.375339</td>\n",
       "      <td>0.799681</td>\n",
       "      <td>0.827544</td>\n",
       "      <td>0.104452</td>\n",
       "      <td>0.946358</td>\n",
       "      <td>4.526811</td>\n",
       "      <td>57.734777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.700056</td>\n",
       "      <td>0.027474</td>\n",
       "      <td>0.262472</td>\n",
       "      <td>1.389359</td>\n",
       "      <td>0.133068</td>\n",
       "      <td>0.071809</td>\n",
       "      <td>0.704375</td>\n",
       "      <td>0.719496</td>\n",
       "      <td>0.026270</td>\n",
       "      <td>0.972629</td>\n",
       "      <td>-73.752772</td>\n",
       "      <td>38.935862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>0.800144</td>\n",
       "      <td>0.010551</td>\n",
       "      <td>0.110018</td>\n",
       "      <td>1.229330</td>\n",
       "      <td>0.055777</td>\n",
       "      <td>0.016889</td>\n",
       "      <td>0.623243</td>\n",
       "      <td>0.631609</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>0.983640</td>\n",
       "      <td>-88.998168</td>\n",
       "      <td>22.932951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>0.900790</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.090652</td>\n",
       "      <td>1.102104</td>\n",
       "      <td>0.045959</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.558743</td>\n",
       "      <td>0.561861</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>0.992764</td>\n",
       "      <td>-90.934760</td>\n",
       "      <td>10.210415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.072937</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036977</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.506978</td>\n",
       "      <td>0.506383</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-92.706296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  cumulative_data_fraction  lower_threshold      lift  \\\n",
       "0         1                  0.010288         0.998292  1.788986   \n",
       "1         2                  0.020337         0.997351  1.612417   \n",
       "2         3                  0.030066         0.996569  1.778458   \n",
       "3         4                  0.040195         0.995519  1.677377   \n",
       "4         5                  0.050004         0.994547  1.828144   \n",
       "5         6                  0.100008         0.989499  1.730238   \n",
       "6         7                  0.150251         0.983800  1.700082   \n",
       "7         8                  0.200973         0.975951  1.730564   \n",
       "8         9                  0.300024         0.942369  1.675489   \n",
       "9        10                  0.400112         0.857930  1.695854   \n",
       "10       11                  0.500040         0.588180  1.597812   \n",
       "11       12                  0.599968         0.159786  1.045268   \n",
       "12       13                  0.700056         0.027474  0.262472   \n",
       "13       14                  0.800144         0.010551  0.110018   \n",
       "14       15                  0.900790         0.005050  0.090652   \n",
       "15       16                  1.000000         0.000026  0.072937   \n",
       "\n",
       "    cumulative_lift  response_rate     score  cumulative_response_rate  \\\n",
       "0          1.788986       0.906977  0.998909                  0.906977   \n",
       "1          1.701740       0.817460  0.997867                  0.862745   \n",
       "2          1.726566       0.901639  0.996884                  0.875332   \n",
       "3          1.714171       0.850394  0.995992                  0.869048   \n",
       "4          1.736530       0.926829  0.995005                  0.880383   \n",
       "5          1.733384       0.877193  0.992002                  0.878788   \n",
       "6          1.722248       0.861905  0.986855                  0.873142   \n",
       "7          1.724347       0.877358  0.979845                  0.874206   \n",
       "8          1.708217       0.849436  0.962482                  0.866029   \n",
       "9          1.705124       0.859761  0.907741                  0.864461   \n",
       "10         1.683679       0.810056  0.741201                  0.853589   \n",
       "11         1.577348       0.529928  0.375339                  0.799681   \n",
       "12         1.389359       0.133068  0.071809                  0.704375   \n",
       "13         1.229330       0.055777  0.016889                  0.623243   \n",
       "14         1.102104       0.045959  0.007356                  0.558743   \n",
       "15         1.000000       0.036977  0.002674                  0.506978   \n",
       "\n",
       "    cumulative_score  capture_rate  cumulative_capture_rate       gain  \\\n",
       "0           0.998909      0.018405                 0.018405  78.898559   \n",
       "1           0.998394      0.016203                 0.034608  61.241701   \n",
       "2           0.997906      0.017304                 0.051911  77.845772   \n",
       "3           0.997423      0.016989                 0.068900  67.737716   \n",
       "4           0.996949      0.017933                 0.086833  82.814412   \n",
       "5           0.994476      0.086519                 0.173352  73.023797   \n",
       "6           0.991927      0.085418                 0.258770  70.008240   \n",
       "7           0.988878      0.087777                 0.346547  73.056443   \n",
       "8           0.980163      0.165959                 0.512506  67.548890   \n",
       "9           0.962047      0.169734                 0.682240  69.585380   \n",
       "10          0.917913      0.159667                 0.841907  59.781194   \n",
       "11          0.827544      0.104452                 0.946358   4.526811   \n",
       "12          0.719496      0.026270                 0.972629 -73.752772   \n",
       "13          0.631609      0.011011                 0.983640 -88.998168   \n",
       "14          0.561861      0.009124                 0.992764 -90.934760   \n",
       "15          0.506383      0.007236                 1.000000 -92.706296   \n",
       "\n",
       "    cumulative_gain  \n",
       "0         78.898559  \n",
       "1         70.173994  \n",
       "2         72.656638  \n",
       "3         71.417148  \n",
       "4         73.652975  \n",
       "5         73.338386  \n",
       "6         72.224802  \n",
       "7         72.434693  \n",
       "8         70.821676  \n",
       "9         70.512418  \n",
       "10        68.367884  \n",
       "11        57.734777  \n",
       "12        38.935862  \n",
       "13        22.932951  \n",
       "14        10.210415  \n",
       "15         0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Raúl/Documents/Doctorado/curso NLP/FINAL-NLP-COURSE/FINAL_NLP_COURSE/Scripts Phd/oboe/notebooks/..objects/amazon/xgb_model_topic/XGBoost_model_python_1594832434245_5585.zip'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.save_mojo('..objects/amazon/xgb_model_topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.loc[:,['A','B']], data['Target'], test_size = 0.30)\n",
    "svm = SVC(C=10**2 ,gamma=10**-4,probability=True)\n",
    "\n",
    "\n",
    "svmlda = svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svmlda.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6317 1129]\n",
      " [ 525 7029]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88      7446\n",
      "           1       0.86      0.93      0.89      7554\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nota: this is a previous evaluation metric that helps us to infer how the topics are distributed again the first target, but must not be considered along the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/svmldatmp.final']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svmlda,'../objects/amazon/svmldatmp.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../objects/amazon/y_a_predecir']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(corpus.iloc[:,1],'../objects/amazon/y_a_predecir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x3468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1209577 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de anomalias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadimos probabilidad de pertenencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "LDA = joblib.load('../objects/amazon/LDA.final')\n",
    "dtm = joblib.load('../objects/amazon/dtm.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = LDA.fit_transform(dtm)\n",
    "corpus['Topic']=topic_results.argmax(axis=1)\n",
    "corpus['prob_topic']=topic_results[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_pickle('../data/processed/amazon/corpus.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Topic</th>\n",
       "      <th>prob_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book recommend friend finally read year late wow wait start addres...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need question know child love impair identify mental illness behav...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keep interest way definitely read jacob s mother teacher father da...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c w gortner favorite author time wow queen release day novel local...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love book like jk alex delaware series problem ending closure oh w...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            cleaned_text  \\\n",
       "0  book recommend friend finally read year late wow wait start addres...   \n",
       "1  need question know child love impair identify mental illness behav...   \n",
       "2  keep interest way definitely read jacob s mother teacher father da...   \n",
       "3  c w gortner favorite author time wow queen release day novel local...   \n",
       "4  love book like jk alex delaware series problem ending closure oh w...   \n",
       "\n",
       "   Target  Topic  prob_topic  \n",
       "0       1      1    0.996960  \n",
       "1       1      1    0.996156  \n",
       "2       1      1    0.995366  \n",
       "3       1      1    0.998420  \n",
       "4       1      1    0.995873  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q3= np.percentile(corpus.loc[corpus.Topic==1,'prob_topic'],[25,75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = q3-q1\n",
    "lower_bound = q1 -(1.5 * iqr) \n",
    "upper_bound = q3 +(1.5 * iqr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943309095570705"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970910136602333"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027601041031628526"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9901907534023262"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0012311698149776"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Topic</th>\n",
       "      <th>prob_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book recommend friend finally read year late wow wait start addres...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need question know child love impair identify mental illness behav...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keep interest way definitely read jacob s mother teacher father da...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c w gortner favorite author time wow queen release day novel local...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love book like jk alex delaware series problem ending closure oh w...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            cleaned_text  \\\n",
       "0  book recommend friend finally read year late wow wait start addres...   \n",
       "1  need question know child love impair identify mental illness behav...   \n",
       "2  keep interest way definitely read jacob s mother teacher father da...   \n",
       "3  c w gortner favorite author time wow queen release day novel local...   \n",
       "4  love book like jk alex delaware series problem ending closure oh w...   \n",
       "\n",
       "   Target  Topic  prob_topic  \n",
       "0       1      1    0.996960  \n",
       "1       1      1    0.996156  \n",
       "2       1      1    0.995366  \n",
       "3       1      1    0.998420  \n",
       "4       1      1    0.995873  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['outlier_iqr']=(corpus.Topic==1) & (corpus.prob_topic<lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1837"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.outlier_iqr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reorganización del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['new_target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[corpus.outlier_iqr==True,'new_target']=2\n",
    "corpus.loc[corpus.Topic==1 &(corpus.outlier_iqr==False),'new_target']=1\n",
    "corpus.loc[(corpus.Topic==0) & (corpus.outlier_iqr==False),'new_target']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no han existido outliers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/amazon/corpus_reordered.final.pck']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(corpus,'../data/processed/amazon/corpus_reordered.final.pck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc8a82ab150>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_b2f0 closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
