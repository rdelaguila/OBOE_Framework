{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .boolean { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .integer { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .string  { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lark import Lark, Transformer\n",
    "import joblib \n",
    "import rulesets\n",
    "import importlib.util\n",
    "importlib.reload(rulesets)\n",
    "\n",
    "from rulesets import *\n",
    "from nlp_explanation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config/grammar.txt') as grammar_file:\n",
    "    grammar = grammar_file.read()\n",
    "parser = Lark(grammar)\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = joblib.load('../objects/amazon/rules_3_experiment/rule_3_5')\n",
    "decorador = RuleStringDecorator(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"if story==5 and ((character>=5 and character<=6) then confidence = 0.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " #or read==10 then confidence = 0.9\"#decorador.getString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = decorador.getString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if character==6 then confidence = 0.006549339577768834'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parser.parse(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@lark.v_args(inline=True)\n",
    "class myStack:\n",
    "    def __init__(self):\n",
    "        self.container = []  # You don't want to assign [] to self - when you do that, you're just assigning to a new local variable called `self`.  You want your stack to *have* a list, not *be* a list.\n",
    "        \n",
    "    def isEmpty(self):\n",
    "        return self.size() == 0   # While there's nothing wrong with self.container == [], there is a builtin function for that purpose, so we may as well use it.  And while we're at it, it's often nice to use your own internal functions, so behavior is more consistent.\n",
    "\n",
    "    def push(self, item):\n",
    "        self.container.append(item)  # appending to the *container*, not the instance itself.\n",
    "\n",
    "    def pop(self):\n",
    "        if self.isEmpty():\n",
    "            return ''\n",
    "        return self.container.pop()  # pop from the container, this was fixed from the old version which was wrong\n",
    "    \n",
    "    def peek(self):\n",
    "        if self.isEmpty():\n",
    "            raise Exception(\"Stack empty!\")\n",
    "        return self.container[-1]  # View element at top of the stack\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.container)  # length of the container\n",
    "\n",
    "    def show(self):\n",
    "        return self.container  # display the entire stack as list\n",
    "from queue import SimpleQueue\n",
    "\n",
    "class MyTransformer(Transformer):\n",
    "\n",
    "    def __init__(self,positive_class=''):\n",
    "        self.nplist = []\n",
    "        self.items_visitados = []\n",
    "        self.last_visited=myStack()\n",
    "        self.cont = 0\n",
    "        self.explicacion = ''\n",
    "        self.relevancia1=''\n",
    "        self.relevancia2=''\n",
    "        self.positive_class=positive_class\n",
    "        self.message = ''\n",
    "    def start(self, items):\n",
    "        caracter = 'The rule states that if the importance/relevance of the terms are as follows: '\n",
    "        int = 1\n",
    "        while self.last_visited.isEmpty() == False:\n",
    "        \n",
    "            caracter+=self.last_visited.pop()+' '\n",
    "        \n",
    "        if \"is at levels ranging from moderately relevant to moderately relevant\" in caracter:\n",
    "            caracter=caracter.replace('is at levels ranging from moderately relevant to moderately relevant',' is moderately relevant')\n",
    "        elif \"is at levels ranging from not relevant to not relevant\" in caracter:\n",
    "            caracter=caracter.replace(\"is at levels ranging from not relevant to not relevant\", \"is not relevant\")\n",
    "        elif \"is at levels ranging from not very relevant to not very relevant\" in caracter:\n",
    "            caracter=caracter.replace(\"is at levels ranging from not very relevant to not very relevant\", \"is not very relevant\")\n",
    "        elif \"is at levels ranging from relevant to relevant\" in caracter:\n",
    "            caracter=caracter.replace(\"is at levels ranging from relevant to relevant\",\"is relevant\")\n",
    "        elif \"is at levels ranging from very relevant to very relevant\" in caracter:\n",
    "            caracter=caracter.replace(\"is at levels ranging from very relevant to very relevant\",\"is very relevant\")\n",
    "        elif \"is at levels ranging from very at maximum level of relevance to at maximum level of relevance\" in caracter:\n",
    "            caracter=caracter.replace(\"is at levels ranging from very at maximum level of relevance to at maximum level of relevance\",\"is at maximum level of relevance\")\n",
    "        else:\n",
    "            caracter+=\", then the text identified by this rule is likely to belong to the possitive class \"+self.positive_class\n",
    "        print(caracter)\n",
    "        \n",
    "        self.message = caracter\n",
    "        \n",
    "#     def expresion_compuesta(self,items):\n",
    "#         for item in items:\n",
    "#             if \n",
    "\n",
    "    def devuelveMensaje(self):\n",
    "        return self.message\n",
    "    \n",
    "    def expresion_and(self,items):\n",
    "        self.cont = self.cont + 1\n",
    "        expresion2 = self.last_visited.pop()\n",
    "        expresion1 = self.last_visited.pop()\n",
    "        #print (expresion1 + ' _ Y _ '+ expresion2)\n",
    "        print('\\nEXPRESION AND '+str(self.cont)+': '+expresion2 + ' _ Y _ '+ expresion1)\n",
    "\n",
    "        if len(expresion1)>0:\n",
    "            \n",
    "            if expresion2.startswith('to') or expresion2.startswith('(to'):\n",
    "                if expresion2.startswith('('):\n",
    "                    expresion2 = expresion2[1:]\n",
    "                    expresion2 = expresion2[:-1]\n",
    "                self.last_visited.push(''+expresion1 + ' '+ expresion2+'')\n",
    "            else:    \n",
    "                self.last_visited.push(''+expresion1 + ' and '+ expresion2+'')\n",
    "        else:\n",
    "            self.last_visited.push(expresion2)        \n",
    "        \n",
    "        self.relevancia1=''\n",
    "        self.relevancia2=''\n",
    "        #self.last_visited.put( 'AND',block=False )\n",
    "      \n",
    "    def expresion_or (self,items):\n",
    "        self.cont = self.cont + 1\n",
    "\n",
    "        expresion2 = self.last_visited.pop()\n",
    "        expresion1 = self.last_visited.pop()\n",
    "\n",
    "        if  len(expresion1)>0:\n",
    "            print('\\n EXPRESION OR '+str(self.cont)+': '+expresion1 + ' or '+ expresion2)\n",
    "            print('range true '+str(self.relevancia1)+'----'+str(self.relevancia2))\n",
    "\n",
    "            if self.relevancia1 == self.relevancia2 and len(self.relevancia1)>0:\n",
    "                print('Haciendo push de '+expresion1)\n",
    "                self.last_visited.push(expresion1)\n",
    "            else:\n",
    "                print('Haciendo push de '+'('+expresion1 + ' or '+ expresion2+')')\n",
    "\n",
    "                self.last_visited.push(''+expresion1 + ' or '+ expresion2+'')\n",
    "        else:\n",
    "            self.last_visited.push(expresion2)\n",
    "\n",
    "        self.relevancia1=''\n",
    "        self.relevancia2=''\n",
    "#print (expresion1 + ' _ O _ '+ expresion2)\n",
    "    def expresion (self, items):\n",
    "        self.cont = self.cont + 1\n",
    "\n",
    "        self.rangeFrom=False\n",
    "        self.rangeTo=False\n",
    "        if items[0]!='(':\n",
    "            termino = items[0]\n",
    "            operator = items[1]\n",
    "            rele = int(str(items[2]))\n",
    "            operador = ''\n",
    "            lpar = ''\n",
    "            rpar=''#items[len(items)-1]\n",
    "            #if items[len(items)-1]=='':\n",
    "                #rpar=''\n",
    "        else:\n",
    "            termino = items[1]\n",
    "            operator = items[2]\n",
    "            rele = int(str(items[3]))\n",
    "            operador = ''\n",
    "            lpar = '('\n",
    "            rpar=''#items[len(items)-1]\n",
    "            #if items[len(items)-1]=='': \n",
    "            #    rpar=''   \n",
    "        print ('\\n'+str(self.cont)+' analizado: ' + termino+operator+str(rele)+'\\n') \n",
    "        if (operator=='>'):\n",
    "            operador = 'is at levels ranging from'\n",
    "            self.rangeFrom = True\n",
    "            self.rangeTo=False\n",
    "            print('range from!!!!')\n",
    "        elif operator=='<' or operator =='<=':\n",
    "            operador='to'\n",
    "            self.rangeTo=True\n",
    "            self.rangeFrom = False\n",
    "            print('range To!!!!!!!')\n",
    "        elif operator=='==':\n",
    "            operador = ' is  '\n",
    "        elif operator=='>=':\n",
    "            operador = 'is at levels ranging from'\n",
    "            self.rangeFrom = True\n",
    "            self.rangeTo = False\n",
    "            print('range from!!!!')\n",
    "\n",
    "        else:\n",
    "            operador = ''\n",
    "        \n",
    "        if operador=='==':\n",
    "            self.last_visited=''\n",
    "        if  rele>=0 and rele<=2:\n",
    "            relevancia = 'not relevant'\n",
    "            if self.rangeFrom==True:\n",
    "                self.relevancia1=copy.deepcopy(relevancia)\n",
    "                print('si estoy metiendo self relevancia1'+self.relevancia1)\n",
    "\n",
    "            if self.rangeTo==True:\n",
    "                self.relevancia2 = copy.deepcopy(relevancia)\n",
    "                \n",
    "        elif rele>=3 and rele<5:\n",
    "            relevancia = 'not very relevant'\n",
    "            if self.rangeFrom==True:\n",
    "                self.relevancia1=copy.deepcopy(relevancia)\n",
    "                print('si estoy metiendo self relevancia1'+self.relevancia1)\n",
    "\n",
    "            if self.rangeTo==True:\n",
    "                self.relevancia2 = copy.deepcopy(relevancia)\n",
    "        elif rele>=5 and rele<7:\n",
    "            relevancia = 'moderately relevant'\n",
    "\n",
    "            if self.rangeFrom==True:\n",
    "                self.relevancia1=copy.deepcopy(relevancia)\n",
    "                print('si estoy metiendo self relevancia1'+self.relevancia1)\n",
    "\n",
    "            if self.rangeTo==True:\n",
    "                self.relevancia2 = copy.deepcopy(relevancia)\n",
    "        elif rele>=6 and rele<8:\n",
    "            relevancia = 'relevant'\n",
    "            if self.rangeFrom==True:\n",
    "                self.relevancia1=copy.deepcopy(relevancia)\n",
    "                print('si estoy metiendo self relevancia1'+self.relevancia1)\n",
    "\n",
    "            if self.rangeTo==True:\n",
    "                self.relevancia2 = copy.deepcopy(relevancia)\n",
    "        \n",
    "        elif rele>=8 and rele<=9:\n",
    "            relevancia = 'very relevant'\n",
    "            if self.rangeFrom==True:\n",
    "                self.relevancia1=copy.deepcopy(relevancia)\n",
    "                print('si estoy metiendo self relevancia1'+self.relevancia1)\n",
    "\n",
    "            if self.rangeTo==True:\n",
    "                self.relevancia2 = copy.deepcopy(relevancia)\n",
    "        else:\n",
    "            relevancia = 'at maximum level of relevance'\n",
    "            if self.rangeFrom==True:\n",
    "                self.relevancia1=copy.deepcopy(relevancia)\n",
    "                print('si estoy metiendo self relevancia1'+self.relevancia1)\n",
    "            if self.rangeTo==True:\n",
    "                self.relevancia2 = copy.deepcopy(relevancia)\n",
    "            #if termino in self.items_visitados:\n",
    "        if operator=='<' or operator =='<=':\n",
    "            if self.rangeTo==True:\n",
    "                if self.relevancia1.strip()==self.relevancia2.strip():\n",
    "                    operador = ' is '\n",
    "                    anterior = self.last_visited.pop()\n",
    "                    if anterior.startswith('('):\n",
    "                        lpar = '('\n",
    "                    \n",
    "                    print('OJITO QUE ME PULO '+anterior)\n",
    "                    self.relevancia2 =''\n",
    "                    print ('Adding simplified to'+ termino + ' '+ operador +' '+ relevancia+')' )\n",
    "                    self.last_visited.push (''+ termino + ' '+ operador +' '+ relevancia+''  )\n",
    "                else:\n",
    "                    print('Adding not simplified: r1'+self.relevancia1+ '- r2:'+self.relevancia2+'::'+lpar+operador +' '+ relevancia+rpar)\n",
    "                    self.last_visited.push (''+operador +' '+ relevancia+'')\n",
    "                \n",
    "        else:\n",
    "            #if rangeFrom==True:\n",
    "            #    rpar=''\n",
    "            #else:\n",
    "            #    rpar=''\n",
    "            print('Adding '+lpar+ termino + ' '+ operador +' '+ relevancia+rpar)\n",
    "            self.last_visited.push (''+ termino + ' '+ operador +' '+ relevancia+''  ) #range from no meter relevancia\n",
    "\n",
    "    def transform(self, tree):\n",
    "        self._transform_tree(tree)\n",
    "        return self.nplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 analizado: book==4\n",
      "\n",
      "Adding book  is   not very relevant\n",
      "\n",
      "2 analizado: read>=7\n",
      "\n",
      "range from!!!!\n",
      "si estoy metiendo self relevancia1relevant\n",
      "Adding read is at levels ranging from relevant\n",
      "\n",
      "3 analizado: read<=8\n",
      "\n",
      "range To!!!!!!!\n",
      "Adding not simplified: r1relevant- r2:very relevant::to very relevant\n",
      "\n",
      "4 analizado: read==10\n",
      "\n",
      "Adding read  is   at maximum level of relevance\n",
      "\n",
      " EXPRESION OR 5: to very relevant or read  is   at maximum level of relevance\n",
      "range true relevant----very relevant\n",
      "Haciendo push de (to very relevant or read  is   at maximum level of relevance)\n",
      "\n",
      "EXPRESION AND 6: to very relevant or read  is   at maximum level of relevance _ Y _ read is at levels ranging from relevant\n",
      "\n",
      "EXPRESION AND 7: read is at levels ranging from relevant to very relevant or read  is   at maximum level of relevance _ Y _ book  is   not very relevant\n",
      "The rule states that if the importance/relevance of the terms are as follows: book  is   not very relevant and read is at levels ranging from relevant to very relevant or read  is   at maximum level of relevance , then the text identified by this rule is likely to belong to the possitive class Book Review\n",
      "The rule states that if the importance/relevance of the terms are as follows: book  is   not very relevant and read is at levels ranging from relevant to very relevant or read  is   at maximum level of relevance , then the text identified by this rule is likely to belong to the possitive class Book Review\n"
     ]
    }
   ],
   "source": [
    "transformer = MyTransformer('Book Review')\n",
    "transformer.transform(tree)\n",
    "print (transformer.devuelveMensaje())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight import *\n",
    "class TextAnalyzer(object):\n",
    "    def __init__(self,nlp):\n",
    "        self.nlp = nlp \n",
    "        \n",
    "    # allow the class instance to be called just like\n",
    "    # just like a function and applies the preprocessing and\n",
    "    # tokenize the document\n",
    "    @staticmethod      \n",
    "    def remove_special_lines(texto):\n",
    "        texto = re.sub(\"^upright=.*[\\r|\\n]\", '', texto)\n",
    "        texto = re.sub(\"^upright = .*[\\r|\\n]\", '', texto)\n",
    "        texto = re.sub(\"Category:.*[\\r|\\n]\",'',texto)\n",
    "        texto = re.sub(\"Cat\\D*:.*[\\r|\\n]\",'',texto)\n",
    "        texto = re.sub(\"[[][\\d]+[]]\",'',texto)\n",
    "        texto = re.sub(\"thumb\",'',texto)\n",
    "        texto = re.sub(\"[|]\",'',texto)\n",
    "        texto = re.sub(\"\\d+px\",'',texto)\n",
    "        return (texto)\n",
    "    @staticmethod\n",
    "    def strip_formatting(string):\n",
    "        string = string.lower()\n",
    "        string = re.sub(r\"([.!?,;-_'/|()]=-<>+*`)\", r\"\", string)\n",
    "        string = re.sub(r'https?:\\/\\/.*?[\\s]', '', string) \n",
    "        return string\n",
    "\n",
    "    def get_nlp(self):\n",
    "        return self.nlp\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        tokens = nlp(doc)\n",
    "        lemmatized_tokens = [(token.lemma_.lower()) for token in tokens\n",
    "                                                   if not (token.is_stop or token.is_punct)]\n",
    "            \n",
    "        return(lemmatized_tokens)\n",
    "    \n",
    "    def is_present (self,word,text):\n",
    "        lemmatized_tokens =  lambda text: \" \".join(token.lemma_.lower() for token in nlp(text) if not (token.is_stop or token.is_punct))\n",
    "        normalizado = lemmatized_tokens(text)    \n",
    "        return (word in (normalizado))\n",
    "\n",
    "class SemanticAnalyzer(TextAnalyzer):\n",
    "    def __init__(self,nlp,endpoint='http://192.168.99.100:2222/rest/annotate/',soporte=1,confianza=0.1,umbral=0.1):\n",
    "        super().__init__(nlp)\n",
    "        self.endpoint = endpoint\n",
    "        self.soporte=soporte\n",
    "        self.confianza = confianza\n",
    "        self.alfa = umbral\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        try:\n",
    "            annotations = spotlight.annotate(self.endpoint,\n",
    "                                     doc,\n",
    "                                      confidence=self.confianza, support=self.soporte, spotter='Default')\n",
    "            diccionario =  dict()\n",
    "            for annotation in annotations:\n",
    "                lista = list(annotation.items())\n",
    "                print(lista)\n",
    "                URI = lista[0]\n",
    "                key = lista[3]\n",
    "                score = lista[5]\n",
    "               # if (score[1]>self.alfa):\n",
    "                diccionario[key[1]]=URI[1]\n",
    "        \n",
    "            return(diccionario)\n",
    "        except Exception as ex:\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos dbpedia y sumo\n",
    "\n",
    "class OntologyOracle(object):\n",
    "    def __init__(self,nlp,dict_onto,dict_graph):\n",
    "\n",
    "        self.dict_onto = dict_onto\n",
    "        self.dict_graph = dict_graph\n",
    "        \n",
    "        self.prefijos = \"\"\"  PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>\n",
    "                                PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "                                PREFIX dbr:    <http://dbpedia.org/resource/>\n",
    "                                PREFIX dbo:    <http://dbpedia.org/ontology/>\n",
    "                                PREFIX dct:    <http://purl.org/dc/terms/>\n",
    "                                PREFIX owl:    <http://www.w3.org/2002/07/owl#>\n",
    "                                PREFIX rdf:    <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "                                PREFIX rdfs:   <http://www.w3.org/2000/01/rdf-schema#>\n",
    "                                PREFIX schema: <http://schema.org/>\n",
    "                                PREFIX skos:   <http://www.w3.org/2004/02/skos/core#>\n",
    "                                PREFIX xsd:    <http://www.w3.org/2001/XMLSchema#>\n",
    "                                PREFIX SUMO: <http://www.adampease.org/OP/SUMO.owl#>\n",
    "                                PREFIX SUMOR: <http://www.ontologyportal.org/SUMO.owl#>\n",
    "                            \"\"\"\n",
    "        self.sa = SemanticAnalyzer(nlp)\n",
    "      \n",
    "    \n",
    "    \n",
    "    def isSubclassOf(self,term1,term2,isDbo):\n",
    "        \n",
    "        consulta = self.prefijos + \"\"\"\n",
    "        ASK  { \n",
    "                \"\"\"+term1+\"\"\" rdfs:subClassOf \"\"\"+term2+\"\"\" }\"\"\"\n",
    "          \n",
    "        if (isDbo==False):\n",
    "            return (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "        else:\n",
    "            return (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "    \n",
    "    \n",
    "    def isSuperclassOf (self,term1,term2,isDbo):\n",
    "        consulta = self.prefijos + \"\"\"\n",
    "        ASK  {  \n",
    "                \"\"\"+term2+\"\"\" rdfs:subClassOf \"\"\"+term1+\"\"\" }\"\"\"\n",
    "          \n",
    "        if (isDbo==False):\n",
    "            return (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "        else:\n",
    "            return (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "    \n",
    "        \n",
    "    def areRelated (self,term1,term2,isDbo):\n",
    "        consulta = self.prefijos + \"\"\"\n",
    "        ASK  {  \"\"\"+term1+\"\"\" ?property \"\"\"+term2+\"\"\" }\"\"\"\n",
    "          \n",
    "        if (isDbo==False):\n",
    "            return (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "        else:\n",
    "            return (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "        \n",
    "    def getRelation(self,term1,term2,isDbo):\n",
    "        consulta = self.prefijos + \"\"\"\n",
    "        select ?property where  {  \"\"\"+term1+\"\"\" ?property \"\"\"+term2+\"\"\" }\"\"\"\n",
    "          \n",
    "        if (isDbo==False):\n",
    "            return (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "        else:\n",
    "            return (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "    \n",
    "    def isDbo (self,term):\n",
    "        res = self.dict_onto.get('dbo').search(label=term,_case_sensitive=False)\n",
    "        if (len(res)>0):\n",
    "            return True\n",
    "        res = self.dict_onto.get('SUMO').search(label=term,_case_sensitive=False)\n",
    "        if (len(res)>0):\n",
    "            return False\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _getBaseConcept(self,term,isDbo):\n",
    "        if (isDbo):\n",
    "            res = self.dict_onto.get('dbo').search(label=term,_case_sensitive=False)\n",
    "            if (len(res)>0):\n",
    "                return str(res[0]).replace('.',':')\n",
    "        \n",
    "        else:\n",
    "            res = self.dict_onto.get('SUMO').search(label=term,_case_sensitive=False)\n",
    "        \n",
    "        if (len(res)>0):\n",
    "            return str(res[0]).replace('.',':')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _getHierarchy(self,concept,isDbo,isSuperclass):\n",
    "        \n",
    "        consulta = self.prefijos + \"\"\"SELECT ?x\n",
    "            WHERE {\n",
    "                ?x a owl:Class .\n",
    "                ?x rdfs:subClassOf \"\"\"+concept+\"\"\"\n",
    "                }\"\"\"\n",
    "        columna = 'superclass'\n",
    "        if isSuperclass == False:\n",
    "            columna='subclass'\n",
    "            consulta = self.prefijos + \"\"\"SELECT ?x\n",
    "            WHERE {\n",
    "                ?x a owl:Class .\n",
    "                \"\"\"+concept+\"\"\" rdfs:subClassOf ?x \n",
    "                }\"\"\"\n",
    "                \n",
    "        if (isDbo==False):\n",
    "            resultado =  (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "        else:\n",
    "            resultado =  (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "           \n",
    "        newresult = [r for res in resultado for r in res ]\n",
    "        print(newresult)\n",
    "        columnas = [columna]\n",
    "        res = pd.DataFrame(data=newresult,columns=columnas)\n",
    "        return newresult\n",
    "    \n",
    "    def _getRelationships(self,concept,isDbo):\n",
    "        consulta = self.prefijos + \"\"\"select distinct  ?property ?value where {\n",
    "                  \"\"\"+concept+\"\"\" ?property ?value .\n",
    "                  filter ( ?property not in ( rdf:type ) )\n",
    "                   filter ( ?property not in ( rdfs:label ) )\n",
    "                   filter ( ?property not in ( rdf:type ) )\n",
    "                   filter ( ?property not in ( rdfs:isDefinedBy ) )\n",
    "                   filter ( ?property not in ( SUMOR:externalImage ) )\n",
    "                   filter ( ?property not in ( SUMOR:axiom ) )\n",
    "                   \n",
    "                  optional {?property rdfs:comment ?comment}\n",
    "                  optional {?property rdfs:range ?range} \n",
    "                  optional {?property rdfs:domain ?domain} \n",
    "                }\n",
    "        \"\"\"\n",
    "        #print(consulta)\n",
    "        if (isDbo==False):\n",
    "            resultado =  (list(self.dict_graph.get('SUMO').query(consulta)))  \n",
    "            res = pd.DataFrame(data = resultado, columns = ['property','element'])\n",
    "            wordnetlist = list()\n",
    "            for wne in res.element:\n",
    "                uri = wne.n3()\n",
    "                uri = uri.replace('http://www.adampease.org/OP/wn','./ontologias/WordNet.owl')\n",
    "                wnconsulta = self.prefijos + \"\"\"select distinct ?label ?comment where {\n",
    "               \"\"\"+uri+\"\"\"  rdf:type owl:Thing .\n",
    "               \"\"\"+uri+\"\"\"   rdfs:label ?label .\n",
    "                \"\"\"+uri+\"\"\"   rdfs:comment ?comment .\n",
    "                \n",
    "                \n",
    "                }\"\"\"\n",
    "                \n",
    "                resultado = self.dict_graph.get('wn').query(wnconsulta)\n",
    "\n",
    "                wordnetlist.append([r for res in resultado for r in res]) \n",
    "            res['explicacion']=wordnetlist\n",
    "\n",
    "        else:\n",
    "            resultado = (list(self.dict_graph.get('dbo').query(consulta)))\n",
    "            res = pd.DataFrame(data = resultado, columns = ['property','element'])\n",
    "\n",
    "        res['term'] = concept\n",
    "        res.set_index(res.term)\n",
    "        return res #(res.to_json())\n",
    "\n",
    "            \n",
    "    def getSemanticsOfTerm(self,term,isDbo=None):\n",
    "        \n",
    "        if isDbo == None:\n",
    "            isDbo = self.isDbo(term)\n",
    "        #if (isDbo):\n",
    "        #    resources = {} #self.sa(term)\n",
    "        #else:\n",
    "        #    resources = {}\n",
    "        if isDbo is None:\n",
    "            #devolvemos semantica de WN\n",
    "            wordnetSemantics = list()\n",
    "            sentidos = wn.synsets(term)\n",
    "            wordnetSemantics.append([(lemma.name(), sysnet.definition()) for sysnet in sentidos for lemma in sysnet.lemmas()])\n",
    "            resultado = pd.DataFrame(data=wordnetSemantics, columns=['termino','significado'])\n",
    "            return False, resultado\n",
    "        \n",
    "        concept = self._getBaseConcept(term,isDbo)\n",
    "        if concept is None:\n",
    "            return None\n",
    "        superclasses = self._getHierarchy(str(concept),isDbo,False)\n",
    "        subclasses = self._getHierarchy(str(concept), isDbo,True)\n",
    "        relationships = self._getRelationships(str(concept),isDbo)\n",
    "        \n",
    "        termino = dict({'concepto':concept,'padres':superclasses,'hijos':subclasses,'relaciones':relationships})\n",
    "        \n",
    "        return True,termino\n",
    "    \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRuleExplainer (object):\n",
    "    \n",
    "    def __init__(self,n_terms=3):\n",
    "        self.n_terms = n_terms\n",
    "    \n",
    "    def construirRelatoTermino(self,alfred,term,isDbo): \n",
    "        relatos=''\n",
    "        sentidos = wn.synsets(term)\n",
    "        if sentidos is None:\n",
    "            relatos +='WordNet does not contains any sense for :'+term+'. Check if it is a domain specific term or is a redaction issue\\n'\n",
    "        else:\n",
    "            relatos += 'Term: '+term+' has been found in WordNet. Retrieving '+str(self.n_terms)+' definitions\\n'\n",
    "            cont = 0\n",
    "            for synset in sentidos:\n",
    "                for lemma in synset.lemmas():\n",
    "                    relatos+='\\t'+str(cont)+'. Lemma sense for '+term+': '+lemma.name()+', Definition: '+ synset.definition()+'\\n'\n",
    "                    cont = cont + 1 \n",
    "                    if cont >= self.n_terms:\n",
    "                        break\n",
    "                if cont >= self.n_terms:\n",
    "                    break\n",
    "                       \n",
    "       \n",
    "        try:\n",
    "            valor,dboTerms = alfred.getSemanticsOfTerm(term,False)\n",
    "\n",
    "            #print(dboTerms)\n",
    "            #print(type(dboTerms))\n",
    "            if dboTerms is not None:\n",
    "                relatos+='\\nThe concept has been found in SUMO ontology: '+dboTerms.get('concepto')+'\\n'\n",
    "\n",
    "                relatos +='Find below the superclasses of the concept:'\n",
    "\n",
    "                elementos = dboTerms.get('padres')\n",
    "                for elemento in elementos:\n",
    "                    print ('\\t'+elemento)\n",
    "\n",
    "        #        print('Hijos: ')\n",
    "        #        mdFile.new_header(level=4, title='Hijos de: '+term)\n",
    "\n",
    "        #        elementos = dboTerms.get('hijos')\n",
    "        #        listah = list()\n",
    "        #        listah = [elemento for elemento in elementos]\n",
    "        #        mdFile.new_list(items=listah)\n",
    "\n",
    "\n",
    "\n",
    "        #        relatos+= 'Some relations have been found: '\n",
    "\n",
    "        #        relaciones = dboTerms.get('relaciones')\n",
    "        #        lista  = [\"Property\", \"Descriptions\"]\n",
    "        #        fila = 1\n",
    "\n",
    "        #        for index, rows in relaciones.iterrows():\n",
    "        #            relatos+='\\t{0}: {1} {2}\\n'.format(str(fila),rows['property'],rows['explicacion'].__str__())\n",
    "        #            fila = fila+1\n",
    "        except:\n",
    "            relatos +='The concept related to term '+term+' was not found in SUMO'\n",
    "\n",
    "      \n",
    "        if isDbo == True:\n",
    "            #relatos+=('\\n********************Retrieving semantics in DBO********************\\n')\n",
    "            try:\n",
    "                valor,dboTerms = alfred.getSemanticsOfTerm(term,True)\n",
    "\n",
    "                #print(dboTerms)\n",
    "                #print(type(dboTerms))\n",
    "                if dboTerms is not None:\n",
    "                    relatos+='\\nThe concept has been found in DBpedia: '+dboTerms.get('concepto')+'\\n'\n",
    "\n",
    "                    relatos +='Find below the superclasses of the concept:'\n",
    "\n",
    "                    elementos = dboTerms.get('padres')\n",
    "                    for elemento in elementos:\n",
    "                        print ('\\t'+elemento)\n",
    "            except:\n",
    "                relatos +='The concept related to term '+term+' was not found in DBpedia'\n",
    "\n",
    "#                relatos+= 'Some relations have been found: '\n",
    "\n",
    "#                relaciones = dboTerms.get('relaciones')\n",
    "#                lista  = [\"Property\", \"Descriptions\"]\n",
    "#                fila = 1\n",
    "\n",
    "#                for index, rows in relaciones.iterrows():\n",
    "#                    relatos+='\\t{0}: {1} {2}\\n'.format(str(fila),rows['property'],rows['explicacion'].__str__())\n",
    "#                    fila = fila+1\n",
    "\n",
    "            \n",
    "        return relatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import time \n",
    "from spacy_langdetect import LanguageDetector\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "myworld1 = World()\n",
    "ontsumo = sumo =myworld1.get_ontology(\"../data/ontologias/SUMO.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontsumo.base_iri=\"http://www.adampease.org/OP/SUMO.owl#\"\n",
    "ontsumo.name='SUMO'\n",
    "sumo =ontsumo.load() #http://www.adampease.org/OP/SUMO.owl\n",
    "graphsumo = myworld1.as_rdflib_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "myworld2 = World()\n",
    "ontdbpedia = dbpedia=myworld2.get_ontology(\"../data/ontologies/dbpedia_2016-10.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * WARNING: ObjectProperty http://dbpedia.org/ontology/senator belongs to more than one entity types: [owl.ObjectProperty, dbpedia_2016-10.MemberOfParliament, DUL.sameSettingAs]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: ObjectProperty http://dbpedia.org/ontology/politicGovernmentDepartment belongs to more than one entity types: [owl.ObjectProperty, dbpedia_2016-10.Department, DUL.hasPart]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: DataProperty http://dbpedia.org/ontology/productShape belongs to more than one entity types: [owl.DatatypeProperty, DUL.hasQuality]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: DataProperty http://dbpedia.org/ontology/latinName belongs to more than one entity types: [owl.DatatypeProperty, dbpedia_2016-10.Name]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: DataProperty http://dbpedia.org/ontology/iso6391Code belongs to more than one entity types: [owl.DatatypeProperty, dbpedia_2016-10.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: DataProperty http://dbpedia.org/ontology/iso6393Code belongs to more than one entity types: [owl.DatatypeProperty, dbpedia_2016-10.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: DataProperty http://dbpedia.org/ontology/iso6392Code belongs to more than one entity types: [owl.DatatypeProperty, dbpedia_2016-10.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: DataProperty http://dbpedia.org/ontology/ingredientName belongs to more than one entity types: [owl.DatatypeProperty, DUL.hasPart]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: DataProperty http://dbpedia.org/ontology/greekName belongs to more than one entity types: [owl.DatatypeProperty, dbpedia_2016-10.Name]; I'm trying to fix it...\n"
     ]
    }
   ],
   "source": [
    "dbpedia = ontdbpedia.load()\n",
    "\n",
    "ontdbpedia.base_iri = \"http://dbpedia.org/ontology/\"\n",
    "ontdbpedia.name='dbo'\n",
    "graphdbo = myworld2.as_rdflib_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "myworld3 = World()\n",
    "ontwordnet = myworld3.get_ontology(\"../data/ontologies/WordNet.owl\")\n",
    "\n",
    "\n",
    "wordnet = ontwordnet.load()\n",
    "ontwordnet.base_iri = \"http://www.adampease.org/OP/WordNet.owl#\"\n",
    "ontwordnet.name='wn'\n",
    "graphwordnet = myworld3.as_rdflib_graph()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx2000M -cp /home/rdelaguila/anaconda3/lib/python3.7/site-packages/owlready2/hermit:/home/rdelaguila/anaconda3/lib/python3.7/site-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////tmp/tmpp0x5ewr6\n",
      "* Owlready2 * HermiT took 2.330838203430176 seconds\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n",
    "dict_onto = dict([('dbo',dbpedia),('SUMO',sumo),('wn',wordnet)])\n",
    "dict_graph = dict([('dbo',graphdbo),('SUMO',graphsumo),('wn',graphwordnet)])\n",
    "sync_reasoner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfred = OntologyOracle(nlp,dict_onto,dict_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nones\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    valor, dboTerms = alfred.getSemanticsOfTerm('read',False)\n",
    "except TypeError:\n",
    "    print('Nones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "platon = SemanticRuleExplainer()\n",
    "relato = platon.construirRelatoTermino(alfred,'read',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: read has been found in WordNet. Retrieving 3 definitions\n",
      "\t0. Lemma sense for read: read, Definition: something that is read\n",
      "\t1. Lemma sense for read: read, Definition: interpret something that is written or printed\n",
      "\t2. Lemma sense for read: read, Definition: have or contain a certain wording or form\n",
      "\t3. Lemma sense for read: read, Definition: look at, interpret, and say out loud something that is written or printed\n",
      "\t4. Lemma sense for read: read, Definition: obtain data from magnetic tapes\n",
      "\t5. Lemma sense for read: read, Definition: interpret the significance of, as of palms, tea leaves, intestines, the sky; also of human behavior\n",
      "\t6. Lemma sense for read: take, Definition: interpret something in a certain way; convey a particular meaning or impression\n",
      "\t7. Lemma sense for read: learn, Definition: be a student of a certain subject\n",
      "\t8. Lemma sense for read: read, Definition: indicate a certain reading; of gauges and instruments\n",
      "\t9. Lemma sense for read: read, Definition: audition for a stage role by reading parts of a role\n",
      "\t10. Lemma sense for read: read, Definition: to hear and understand\n",
      "\t11. Lemma sense for read: understand, Definition: make sense of a language\n",
      "The concept related to term read was not found in SUMO\n"
     ]
    }
   ],
   "source": [
    "print (relato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
